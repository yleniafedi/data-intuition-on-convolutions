{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitions on Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build some intuitions about these **`convolution operations`**.\n",
    "\n",
    "üéØ <b><u>Goals</u>:</b>\n",
    "- **Understand convolution operations**\n",
    "- **Visualize**\n",
    "    - convolution kernels\n",
    "    - the effects of a convolution kernel applied on images\n",
    "\n",
    "<hr>\n",
    "\n",
    "üñº <b><u>Convolutional Neural Networks are Neural Networks specifically designed to work on images</u></b>. \n",
    "\n",
    "- üßÆ This is made possible thanks to **`convolution operations`**.\n",
    "\n",
    "- üîé These specific mathematical operations apply a **`filter`** (i.e. a set of **`kernels`**, one per channel) to an input image and create an **`output representation`**. For Convolutional Neural Networks, this can also be called:\n",
    "    * a **`\"convoluted representation/feature\"`**,\n",
    "    * or a **`\"convolution\"`**,\n",
    "    * or also an **`\"activation\"`** (as it corresponds to the activation of a given layer).\n",
    "\n",
    "<img src=\"convolution.png\" width=\"300\">\n",
    "\n",
    "---\n",
    "\n",
    "‚ùóÔ∏è <b><u>Remarks</u></b> ‚ùóÔ∏è\n",
    "\n",
    "* It is important to understand that **the same kernel (i.e. the same weights) is applied to different areas of the images**. \n",
    "\n",
    "* This is completely different from Dense Neural Networks that we've been working with during the first two units of the Deep Learning module:\n",
    "    * In `Dense/\"Fully Connected\" Neural Networks`, each weight of a given neuron is related to only one input coordinate (which, in images, would correspond to one pixel).\n",
    "    * In `Convolution Neural Networks`, the weights of a kernel are not applied to only one feature input, i.e. one pixel, but to different pixels, \"step by step\"!\n",
    "\n",
    "üëâ You can think of each kernel (or each filter in the case of colored images) as a **`magnifying glass`** through which you see the image. Similarly to your eyes, kernels cannot capture everything in a picture at once, but they ***scan different parts of a picture to understand the whole picture that is being analyzed***.\n",
    "\n",
    "üé¨ So let's have a closer look at `convolution operations`, and their impact in `Convolutional Neural Networks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use the following function `load_data` to... load the data.\n",
    "\n",
    "* Do not change anything in the function!\n",
    "* Restrict from any desire to change the shapes or types of the outputs! This will have an impact on further questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load_data(path):\n",
    "    nb_circles = 100\n",
    "    nb_triangles = 100\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(nb_circles):\n",
    "        c_path = os.path.join(path, 'circles', f'circle_{i}.png')\n",
    "        X.append(imread(c_path)[:, :, :1])\n",
    "        y.append(0)\n",
    "    \n",
    "    for i in range(nb_triangles):\n",
    "        t_path = os.path.join(path, 'triangles', f'triangle_{i}.png')\n",
    "        X.append(imread(t_path)[:, :, :1])\n",
    "        y.append(1)\n",
    "        \n",
    "    c = list(zip(X, y))\n",
    "    np.random.shuffle(c)\n",
    "    X, y = zip(*c)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "        \n",
    "X, y = load_data(\"data\")\n",
    "# Replace data by \"https://wagon-public-datasets.s3.amazonaws.com/deep-learning-circles-triangles/\" \n",
    "# if you are on a server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the shape** ‚ùì\n",
    "\n",
    "* How many images do we have?\n",
    "* What are their dimensions? \n",
    "* Can you comment on the number of channels? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>About the number of channels:</i></summary>\n",
    "   \n",
    "Actually, we have already talked about this during the `MNIST challenge`.        \n",
    "        \n",
    "*  We need only one channel to compute the \"*blackness intensity*\" of a pixel with 0 corresponding to a black pixel and 1 corresponding to a white pixel. The last dimension corresponds to some kind of  \"Black to white channel\". \n",
    "        \n",
    "üé® For colored images, the last dimension would be equal to 3 for `Red, Green, Blue (RGB)`\n",
    "\n",
    "üëâ Have fun playing with the intensities of Red, Green and Blue <a href=\"https://www.w3schools.com/colors/colors_rgb.asp\">`here`</a>\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the normalization** ‚ùì\n",
    "\n",
    "Do these images need some normalization? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ Let's have a look at some images with `plt.imshow` and show their respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_, (image, label) in enumerate(zip(X, y)):\n",
    "    plt.imshow(image[:, :, 0], cmap='gray')\n",
    "    plt.title('Triangle' if label == 1 else 'Circle')\n",
    "    plt.show()\n",
    "    \n",
    "    if iter_ > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many classes are we going to predict** ‚ùì\n",
    "\n",
    "_This information will help you design the last layer of your Convolutional Network_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Kernels & Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ The following function **`compute_convolution`** performs a **convolution operation** $ \\Leftrightarrow $ i.e. *it applies a kernel to an image*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the `compute_convolution` function** ‚ùì\n",
    "\n",
    "Run it and try to understand the different steps of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_convolution(input_image, kernel):\n",
    "    # Parameters\n",
    "    kernel = np.array(kernel)\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    img = np.squeeze(input_image) # Removes dimensions of size 1\n",
    "    img_height, img_width = img.shape\n",
    "    \n",
    "    output_image = []\n",
    "\n",
    "    for x in range(img_height - kernel_height + 1):\n",
    "        arr = []\n",
    "        \n",
    "        for y in range(img_width - kernel_width + 1):\n",
    "            \n",
    "            a = np.multiply(img[x: x + kernel_height, y: y + kernel_width],\n",
    "                            kernel)\n",
    "            arr.append(a.sum())\n",
    "            \n",
    "        output_image.append(arr)\n",
    "        \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **How does the `compute_convolution` function work in practice** ‚ùì \n",
    "\n",
    "1. Choose any image from the input dataset\n",
    "2. Apply the `identity_kernel` to it\n",
    "3. Display both the input image and the output image. \n",
    "4. Do you see any differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_kernel = [\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "        \n",
    "üßëüèª‚Äçüè´ The previous kernel corresponds to the **`identity_kernel`**, meaning that ***the output is equal to the input***... \n",
    "    \n",
    "üïµüèª‚Äç‚ôÇÔ∏è It basically did nothing to the input image. It you think about it thoroughly, that's not surprising. With this kernel, only the pixel scanned in the middle was kept and multiplied by one, the surrounding pixels were multiplied by zero.        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ We coded a function **`plot_convolution`** that plots the output image after applying a kernel to an input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convolution(img, kernel, activation=False):\n",
    "    ''' The following printing function ease the visualization'''\n",
    "    \n",
    "    img = np.squeeze(img)\n",
    "    output_img = compute_convolution(img, kernel)\n",
    "    if activation:\n",
    "        output_img = np.maximum(output_img, 0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    ax1 = plt.subplot2grid((3,3),(0,0), rowspan=3)\n",
    "    ax1.imshow(img, cmap='gray')\n",
    "    ax1.title.set_text('Input image')\n",
    "    \n",
    "    ax2 = plt.subplot2grid((3,3),(1, 1))\n",
    "    ax2.imshow(kernel, cmap='gray')\n",
    "    ax2.title.set_text('Kernel')    \n",
    "    \n",
    "    ax3 = plt.subplot2grid((3,3),(0, 2), rowspan=3)\n",
    "    ax3.imshow(output_img, cmap='gray')\n",
    "    ax3.title.set_text('Output image')    \n",
    "\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Apply `plot_convolution` with the following `kernel_1` once on an triangle and once on a circle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_1 = [\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0],\n",
    "    [-1, -1, -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Let's analyze what just happened:\n",
    "\n",
    "* White pixels correspond to high values and black pixels to low values.\n",
    "* In a Neural Network, remember that we use activation functions to remove linearities. \n",
    "    * *For example*, when the activation function is `relu`, you already know that it simply corresponds to setting the negative values to 0.\n",
    "\n",
    "---\n",
    "\n",
    "‚ùì **What is the impact of the activation function in a Convolutional Layer ?** ‚ùì\n",
    "\n",
    "Re-run the previous function `plot_convolution` with `activation` set to `True` (in this case, the activation function _is_ the relu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ This kernel is actually highlighting the edges in a given direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Play with different kernels...** ‚ùì\n",
    "\n",
    "Try the following kernels to check the different edges they can detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_2 = [\n",
    "    [-1, -1, -1],\n",
    "    [0, 0, 0],   \n",
    "    [1, 1, 1],\n",
    "]\n",
    "\n",
    "kernel_3 = [\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "]\n",
    "\n",
    "kernel_4 = [\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **What is the effect of the kernel size** ‚ùì\n",
    "\n",
    "Try the _kernel_big_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_big = np.array([\n",
    "    np.ones((10,)),\n",
    "    np.ones((10,)),\n",
    "    np.ones((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.ones((10,))*-1,\n",
    "    np.ones((10,))*-1,\n",
    "    np.ones((10,))*-1,\n",
    "])\n",
    "\n",
    "kernel_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Try another kernel**  ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_kernel = np.random.uniform(-10, 10, (5, 5))\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've got the idea of what a convolution operation does to an image, let's see how it goes with a \"real\" Convolutional Neural Network. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Training a CNN to detect triangles and circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: Designing a CNN** ‚ùì\n",
    "\n",
    "Write a Convolutional Network that has \n",
    "- a Convolutional Layer with 16 filters with $ (4, 4) $ kernels.\n",
    "- a Convolutional Layer with 32 filters with $ (3, 3) $ kernels.\n",
    "- a Convolutional Layer with 64 filters with $ (3, 3) $ kernels.\n",
    "- a Convolutional Layer with 64 filters with $ (2, 2) $ kernels.\n",
    "\n",
    "with:\n",
    "- A Max-Pooling Layer (with a $ (2, 2) $ pool-size) after each convolution.\n",
    "- A Hidden Dense Layer with the size of your choice, be reasonable:\n",
    "    - after the flattening part \n",
    "    - but before the last layer\n",
    "\n",
    "\n",
    "Also, make sure to compile your model with the appropriate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def initialize_model():\n",
    "    \n",
    "    pass  # YOUR CODE HERE    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Training the CNN** ‚ùì\n",
    "\n",
    "* Fit the model. You should achieve a accuracy of *at least* 90%. \n",
    "\n",
    "    * When you reach such a high score, it may sound suspicious and you would probably ask yourself whether the model is overfitting or not... but let's ignore it for this challenge üòè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ We coded the `plot_loss_accuracy` for you.\n",
    "\n",
    "‚ùì **Question: does the CNN converge** ‚ùì\n",
    "\n",
    "_Also, do you see any sign of overfitting?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history, title=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    \n",
    "    # --- LOSS --- \n",
    "    \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- ACCURACY\n",
    "    \n",
    "    ax[1].plot(history.history['accuracy'])\n",
    "    ax[1].plot(history.history['val_accuracy'])\n",
    "    ax[1].set_title('Model Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëè Congratulations! After running a CNN to classify handwritten digits, you were able to design another to classify images either as triangles or circles!\n",
    "\n",
    "üòè However, you have probably guessed that Computer Vision is obviously more complex than that...\n",
    "\n",
    "üìÜ In `Challenge 3 - CIFAR10`, you will try to classify images between 10 categories.\n",
    "\n",
    "üìö The last section of this notebook will help you build a deeper understanding of CNN. **Read `(2.3) Deeper understanding of CNN` very carefully** before moving on the next challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéÅ üìö (2.3) Deeper understanding of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë©üèª‚Äçüè´ For any Sequential Neural Network (Dense or Convolutional), you can:\n",
    "- print the **`.summary()`** to display the layers and the number of weights/parameters involved\n",
    "- access the different **`.layers`** of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üïµÔ∏è‚Äç‚ôÄÔ∏è With the following table, you will have a better overview of the different weights, kernels and filters involved in the CNN you've built earlier:\n",
    "\n",
    "| layer_number | convolution_layer | kernel_number | channel_number |\n",
    "|--------------|-------------------|---------------|----------------|\n",
    "| 0            | conv2D no 1       | 16            | 1              |\n",
    "| 2            | conv2D no 2       | 32            | 16             |\n",
    "| 4            | conv2D no 3       | 64            | 32             |\n",
    "| 6            | conv2D no 4       | 64            | 64             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3.1) Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßëüèª‚Äçüè´ It is possible to **retrieve the values of all the kernels for each layer after training a CNN**. \n",
    "\n",
    "üëâ Let's focus first on the different parameters (**`.weights`**) of the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the first convolutional layer of the CNN\n",
    "first_convolutional_layer = model.layers[0]\n",
    "first_convolutional_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of the first convolutional layer of the CNN - which was trained/optimized\n",
    "first_convolutional_layer.weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ `TensorShape[4, 4, 1, 16]` represents:\n",
    "- the weights of each kernel (size `4` $\\times$ `4`)\n",
    "- there was only `1` channel (single B&W input)\n",
    "- and we have decided to apply `16` different kernels in this layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biases of the first convolutional layer of the CNN - which was trained/optimized\n",
    "first_convolutional_layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Let's not forget the biases, one per new channel in the output image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we have indeed 256 weights + 16 biases = 272 parameters for the first convolutional layer\n",
    "4*4*1*16+16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the second convolutional layer of the CNN\n",
    "second_convolutional_layer = model.layers[2]\n",
    "second_convolutional_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of the second convolutional layer of the CNN - which was trained/optimized\n",
    "second_convolutional_layer.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biases of the second convolutional layer of the CNN - which was trained/optimized\n",
    "second_convolutional_layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we have indeed 4608 weights + 32 biases = 4640 parameters for the third convolutional layer\n",
    "3*3*16*32+32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the third convolutional layer of the CNN\n",
    "third_convolutional_layer = model.layers[4]\n",
    "third_convolutional_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of the third convolutional layer of the CNN - which was trained/optimized\n",
    "third_convolutional_layer.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biases of the third convolutional layer of the CNN - which was trained/optimized\n",
    "third_convolutional_layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we have indeed 18432 weights + 64 biases = 18496 parameters for the fourth convolutional layer\n",
    "3*3*32*64+64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the fourth convolutional layer of the CNN\n",
    "fourth_convolutional_layer = model.layers[6]\n",
    "fourth_convolutional_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of the fourth convolutional layer of the CNN - which was trained/optimized\n",
    "fourth_convolutional_layer.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biases of the fourth convolutional layer of the CNN - which was trained/optimized\n",
    "fourth_convolutional_layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we have indeed 16384 weights + 64 biases = 16448 parameters for the fourth convolutional layer\n",
    "(2*2*64+1)*64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ Now that we know how to access the trained weights of each kernel for every convolutional layer, we are going to investigate how they impact the analysis of images.\n",
    "\n",
    "* ü™Ñ Using **`plot_convolution(activation = True)`**, let's display the 16 kernels from the first convolutional layer, alongside with the activation output, to see what the model has learned from the images in this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3.2) Activations üß®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been looking at the activation (\"_output image_\") only after the **first convolutional layer**.\n",
    "\n",
    "ü§î What if we want to **visualize the activation of an image after every convolutional layer of the CNN** ?\n",
    "\n",
    "* üìö We are going to use the [**Functional API**](https://www.tensorflow.org/guide/keras/functional) from Tensorflow/Keras.\n",
    "\n",
    "* üí™ Stay with us, this is the hardest part of the notebook but also the last one. And we are just asking you to stay focused and read üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 0Ô∏è‚É£ : Reminders of the CNN's summary***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 1Ô∏è‚É£: listing all the 11 layers' outputs of your CNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_outputs = [layer.output for layer in model.layers]\n",
    "layers_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 2Ô∏è‚É£ : Instantiate 11 sub-models re-using already trained weights and biases***\n",
    "- layer1\n",
    "- layer1 $ \\Rightarrow $ layer2\n",
    "- layer1 $ \\Rightarrow $ layer2 $ \\Rightarrow $ layer3\n",
    "- ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "activation_models = [Model(inputs=model.input, outputs=output) for output in layers_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 3Ô∏è‚É£ : Compute the outputs of each submodel***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [m.predict(X) for m in activation_models]\n",
    "len(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[activation.shape for activation in activations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòé PRO TIP:\n",
    "* In Tensorflow, you can also create a single model with many outputs to avoid Python loops / list comprehensions\n",
    "* Steps 1Ô∏è‚É£ 2Ô∏è‚É£ 3Ô∏è‚É£ can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_outputs = [layer.output for layer in model.layers] # same as above\n",
    "activation_model = Model(inputs=model.input, outputs=layers_outputs) # model with many outputs !\n",
    "activations = activation_model.predict(X) # 11 predictions at once!\n",
    "[activation.shape for activation in activations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3.3) The final show ü•Å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî• Now that the activations are computed, we can choose one image in the dataset and observe the different \"activation images\" through each convolutional layer! In other terms, we are now able to observe what the CNN sees for each image! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* üëá Run the code down below and observe how a triangle was seen through the different convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a random triangle\n",
    "image_number = np.random.choice(np.where(y == 1)[0]) \n",
    "\n",
    "for layer_number in [0,2,4,6]:\n",
    "    \n",
    "    print(f\"--- Observing the effect of the convolutional layer number {layer_number}... ---\")\n",
    "    print(\"\")\n",
    "    \n",
    "    temp_number_kernels = model.layers[layer_number].weights[0].shape[-1]\n",
    "    print(f\"{temp_number_kernels} kernels were applied and here are all the activations of this Conv2D Layer:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(int(temp_number_kernels/4),4, figsize=(20,7))\n",
    "    \n",
    "    \n",
    "    for ax, kernel_number in zip(axes.flat,range(temp_number_kernels)):\n",
    "        activation = activations[layer_number][image_number][:, :, kernel_number]\n",
    "        ax.imshow(activation, cmap=\"gray\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* üëá Run the code down below and observe how a circle) was seen through the different convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a random triangle\n",
    "image_number = np.random.choice(np.where(y == 0)[0]) \n",
    "\n",
    "for layer_number in [0,2,4,6]:\n",
    "    \n",
    "    print(f\"--- Observing the effect of the convolutional layer number {layer_number}... ---\")\n",
    "    print(\"\")\n",
    "    \n",
    "    temp_number_kernels = model.layers[layer_number].weights[0].shape[-1]\n",
    "    print(f\"{temp_number_kernels} kernels were applied and here are all the activations of this Conv2D Layer:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(int(temp_number_kernels/4),4, figsize=(20,7))\n",
    "    \n",
    "    \n",
    "    for ax, kernel_number in zip(axes.flat,range(temp_number_kernels)):\n",
    "        activation = activations[layer_number][image_number][:, :, kernel_number]\n",
    "        ax.imshow(activation, cmap=\"gray\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßëüèª‚Äçüè´ ***Notes*** üßëüèª‚Äçüè´\n",
    "\n",
    "1. Notice how the information of an image **flows** through the Convolutional Neural Network.\n",
    "2. You should see the picture becoming more and more \"abstract\", of smaller and smaller \"dimensions\"\n",
    "\n",
    "üïπ Feel free to play with the [CNN Explainer](https://poloclub.github.io/cnn-explainer/) from researchers at [Georgia Tech](https://www.gatech.edu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Appendix) Utils\n",
    "\n",
    "* The following section simply presents the functions that helped us create the dataset with triangles and circles you have been working with.\n",
    "\n",
    "* They were left at the end of the notebook just in case you want to further prototype and get better understanding of what is going on. \n",
    "\n",
    "* But ***for this first day discovering Computer Vision, skip this section and go to the next exercise***, you can always come back to it any time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_triangle():\n",
    "    dx = np.random.uniform(0.1, 0.3)\n",
    "    dy = np.random.uniform(0.1, 0.3)\n",
    "    noise_x = np.random.uniform(0.0, 0.1)\n",
    "    noise_y = np.random.uniform(0.0, 0.1)    \n",
    "    \n",
    "    x = np.random.uniform(0, 1-dx-noise_x)\n",
    "    y = np.random.uniform(0, 1-dy)\n",
    "    X = np.array([[x,y], [x+dx+noise_x,y], [x+dx/2, y+dy+noise_y]])\n",
    "\n",
    "    t1 = plt.Polygon(X, color='black')\n",
    "    plt.gca().add_patch(t1)\n",
    "    \n",
    "def draw_circle():\n",
    "    r = np.random.uniform(0.1, 0.25)\n",
    "    x = np.random.uniform(0+r, 1-r)\n",
    "    y = np.random.uniform(0+r, 1-r)\n",
    "\n",
    "    circle1 = plt.Circle((x, y), r, color='black')\n",
    "    plt.gcf().gca().add_artist(circle1)\n",
    "    \n",
    "def create_image(form, path):\n",
    "    plt.figure(figsize=(1, 1))\n",
    "    if form == 'circle':\n",
    "        draw_circle()\n",
    "    elif form == 'triangle':\n",
    "        draw_triangle()\n",
    "    plt.axis('off')\n",
    "    plt.savefig(path, dpi=80, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def create_images(path):\n",
    "    nb_circles = 100\n",
    "    nb_triangles = 100\n",
    "    \n",
    "    for i in range(nb_circles):\n",
    "        c_path = os.path.join(path, 'circles', f'circle_{i}.png')\n",
    "        create_image('circle', c_path)\n",
    "        \n",
    "    for i in range(nb_triangles):\n",
    "        t_path = os.path.join(path, 'triangles', f'triangle_{i}.png')\n",
    "        create_image('triangle', t_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
